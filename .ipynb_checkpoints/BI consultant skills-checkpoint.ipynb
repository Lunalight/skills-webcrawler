{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from goose3 import Goose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "program_languages=['bash','r','python','java','c++','c#', 'f#', 'ruby','perl','matlab','javascript','scala','php']\n",
    "analysis_software=['alteryx', 'excel','tableau', 'power bi', 'powerbi', 'qlikview', 'qlik','d3.js','sas','sap','spss','d3','saas','pandas','numpy','scipy','sps','spotfire','scikits.learn','splunk','powerpoint','h2o']\n",
    "bigdata_tool=['hadoop','mapreduce','spark','pig','hive','shark','oozie','zookeeper','flume','mahout']\n",
    "databases=['sql','nosql','hbase','cassandra','mongodb','mysql','mssql','postgresql','oracle db','rdbms']\n",
    "machine_learning = ['keras', 'scikit-learn', 'tensorflow', 'caffe', 'datarobot', 'theano', 'torch']\n",
    "overall_dict = program_languages + analysis_software + bigdata_tool + databases + machine_learning\n",
    "\n",
    "communicative_skills = [\"analytisch vermogen\", \"projectmanagement\", \"proactief\", \"communicatieve vaardigheden\", \"uitdrukkingsvaardigheden\", \"agile\", \"scrum\", \n",
    "\"advies\", \"adviseur\", \"werkt samen\", \"werkt nauw samen\", \"samenwerken\", \"analytisch\", \"planmatig\", \"zelfstandig\", \"enthousiast\", \"vertalen\", \"inventariseren\", \"analyseren\", \n",
    "\"realiseren\", \"ondernemen\", \"adviseer\", \"oplossingsgericht\", \"klantgericht\", \"gedreven\", \"creatief\", \"meedenkend\", \"resultaatgericht\", \"oplossingsgericht\",\n",
    "\"prioriteren\", \"presenteren\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keywords_extract(url):\n",
    "    g = Goose()\n",
    "    article = g.extract(url=url)\n",
    "    text = article.cleaned_text\n",
    "    text = re.sub(\"[^a-zA-Z+3]\",\" \", text) #get rid of things that aren't words; 3 for d3 and + for c++\n",
    "    text = text.lower().split()\n",
    "    stops = set(stopwords.words(\"english\")) #filter out stop words in english language\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = list(set(text))\n",
    "    keywords = [str(word) for word in text if word in overall_dict]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keywords_f(soup_obj):\n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "    text = soup_obj.get_text() \n",
    "    lines = (line.strip() for line in text.splitlines()) # break into line\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) # break multi-headlines into a line each\n",
    "    text = ''.join(chunk for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends of line\n",
    "    try:\n",
    "        text = text.decode('utf-8') # Need this as some websites aren't formatted\n",
    "    except:                                                          \n",
    "        return                                                       \n",
    "    text = re.sub(\"[^a-zA-Z+3]\",\" \", text)  \n",
    "    text = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", text) # Fix spacing issue from merged words\n",
    "    text = text.lower()\n",
    "    text = re.sub(('power bi'), 'powerbi', text) # catch power bi before tokenization\n",
    "    text = text.split()  \n",
    "    stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "    text = list(set(text)) #only care about if a word appears, don't care about the frequency\n",
    "    keywords = [str(word) for word in text if word in overall_dict] #if a skill keyword is found, return it.\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def communication_skills_f(soup_obj):\n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "    text = soup_obj.get_text() \n",
    "    lines = (line.strip() for line in text.splitlines()) # break into line\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) # break multi-headlines into a line each\n",
    "    text = ''.join(chunk for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends of line\n",
    "    try:\n",
    "        text = text.decode('utf-8') # Need this as some websites aren't formatted\n",
    "    except:                                                          \n",
    "        return                                                       \n",
    "    text = re.sub(\"[^a-zA-Z+3]\",\" \", text)  \n",
    "    text = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", text) # Fix spacing issue from merged words\n",
    "    text = text.lower()\n",
    "    skills = [word for word in communicative_skills if word in text] \n",
    "    return skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 803 jobs found and we need to extract 80 pages.\n",
      "extracting first page of job searching results\n"
     ]
    }
   ],
   "source": [
    "base_url = \"http://www.indeed.nl\"    \n",
    "#change the start_url can scrape different cities.\n",
    "start_url = \"http://www.indeed.nl/jobs?q=BI+consultant&l=\"\n",
    "resp = requests.get(start_url)\n",
    "start_soup = BeautifulSoup(resp.content, 'lxml', from_encoding='utf-8')\n",
    "urls=start_soup.find_all('a',{'rel':'nofollow','target':'_blank'})\n",
    "urls = [link['href'] for link in urls] \n",
    "num_found = start_soup.find(id = 'searchCount').string.encode('utf-8').split() #this returns the total number of results\n",
    "num_jobs = num_found[-1].decode('utf-8').split(',')\n",
    "num_jobs = [x.replace('.', '') for x in num_jobs]\n",
    "\n",
    "if len(num_jobs)>=2:\n",
    "    num_jobs = int(num_jobs[0]) * 1000 + int(num_jobs[1])\n",
    "else:\n",
    "    num_jobs = int(num_jobs[0])\n",
    "num_pages = int(num_jobs/10) #calculates how many pages needed to do the scraping\n",
    "\n",
    "job_keywords=[]\n",
    "print('There are %d jobs found and we need to extract %d pages.'%(num_jobs,num_pages))\n",
    "print('extracting first page of job searching results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting 0 job keywords...\n",
      "['agile', 'vertalen', 'inventariseren', 'analyseren', 'realiseren', 'ondernemen', 'gedreven', 'resultaatgericht'] 1\n",
      "https://www.indeed.nl/vacature-bekijken?jk=638eb3aa8de6ce6f&tk=1ckual13h9nordo6&from=serp&alid=3&advn=6124692058421937\n",
      "extracting 1 job keywords...\n",
      "['analytisch', 'zelfstandig', 'enthousiast', 'vertalen', 'analyseren', 'klantgericht', 'creatief'] 1\n",
      "https://www.indeed.nl/vacature-bekijken?jk=209af06ed43c644b&tk=1ckual3df9nor99h&from=serp&alid=3&advn=2421485629829485\n",
      "extracting 2 job keywords...\n",
      "[] 1\n",
      "https://www.indeed.nl/vacature-bekijken?jk=7593cc50f485ec4f&tk=1ckual5k99norbur&from=serp&alid=3&advn=2421485629829485\n",
      "extracting 3 job keywords...\n",
      "['scrum', 'advies', 'adviseur', 'werkt samen', 'gedreven'] 1\n",
      "https://www.indeed.nl/vacature-bekijken?jk=24115a299a2e1531&tk=1ckual7n49norem0&from=serp&alid=3&advn=8245509577778884\n",
      "extracting 4 job keywords...\n",
      "['agile', 'inventariseren', 'analyseren', 'realiseren', 'ondernemen', 'adviseer'] 1\n",
      "https://www.indeed.nl/vacature-bekijken?jk=06b8129ce00d51e0&from=serp&vjs=3\n",
      "extracting 5 job keywords...\n",
      "['gedreven'] 1\n",
      "https://www.indeed.nl/vacature-bekijken?jk=6f4023c70f7edeb2&from=serp&vjs=3\n",
      "extracting 6 job keywords...\n",
      "['zelfstandig'] 1\n",
      "https://www.indeed.nl/vacature-bekijken?jk=ee47624aa5b18954&from=serp&vjs=3\n",
      "extracting 7 job keywords...\n",
      "['proactief', 'communicatieve vaardigheden', 'advies', 'werkt nauw samen', 'analytisch', 'planmatig'] 1\n",
      "https://www.indeed.nl/vacature-bekijken?jk=69e2493bfb1ad195&from=serp&vjs=3\n",
      "extracting 8 job keywords...\n",
      "['advies', 'adviseur', 'klantgericht'] 1\n",
      "https://www.indeed.nl/vacature-bekijken?jk=e0771b58557a838a&from=serp&vjs=3\n",
      "extracting 9 job keywords...\n",
      "['communicatieve vaardigheden', 'vertalen', 'klantgericht'] 1\n",
      "https://www.indeed.nl/vacature-bekijken?jk=39a4a955833d121d&from=serp&vjs=3\n",
      "extracting 10 job keywords...\n",
      "['zelfstandig', 'enthousiast', 'vertalen', 'creatief'] 1\n",
      "https://www.indeed.nl/vacature-bekijken?jk=42d833f3b8b88de1&from=serp&vjs=3\n",
      "extracting 11 job keywords...\n",
      "['adviseur', 'analytisch', 'vertalen', 'oplossingsgericht', 'klantgericht', 'gedreven', 'oplossingsgericht'] 1\n",
      "https://www.indeed.nl/vacature-bekijken?jk=4b20794365970d75&from=serp&vjs=3\n",
      "extracting 12 job keywords...\n",
      "['analytisch vermogen', 'communicatieve vaardigheden', 'agile', 'analytisch', 'inventariseren', 'analyseren', 'realiseren', 'ondernemen', 'adviseer', 'resultaatgericht'] 1\n",
      "https://www.indeed.nl/vacature-bekijken?jk=f4d29a2daf1cc2f2&from=serp&vjs=3\n",
      "extracting 13 job keywords...\n",
      "['agile', 'scrum', 'klantgericht'] 1\n",
      "https://www.indeed.nl/vacature-bekijken?jk=7388e98473ad2585&from=serp&vjs=3\n",
      "extracting 14 job keywords...\n",
      "[] 1\n",
      "https://www.indeed.nl/vacature-bekijken?jk=9a8fdc01f707b1fe&tk=1ckuam39t9norb0b&from=serp&alid=3&advn=713903982049763\n",
      "extracting 15 job keywords...\n",
      "['advies', 'werkt samen', 'zelfstandig', 'enthousiast'] 1\n",
      "https://www.indeed.nl/vacature-bekijken?jk=0a8fa72b3f5e0492&tk=1ckuam5q49nor9io&from=serp&alid=3&advn=2180790811884654\n"
     ]
    }
   ],
   "source": [
    "# prevent the driver stopping due to the unexpectedAlertBehaviour.\n",
    "webdriver.DesiredCapabilities.FIREFOX[\"unexpectedAlertBehaviour\"] = \"accept\"\n",
    "get_info = True\n",
    "driver = webdriver.Firefox(executable_path=r'C:\\Users\\emily\\Documents\\ML course\\geckodriver.exe')\n",
    "# set a page load time limit so that don't have to wait forever if the links are broken.\n",
    "driver.set_page_load_timeout(15)\n",
    "for i in range(len(urls)):\n",
    "    get_info = True\n",
    "    try:\n",
    "        driver.get(base_url+urls[i])\n",
    "    except TimeoutException:\n",
    "        get_info = False\n",
    "        continue\n",
    "    j = random.randint(1000,2200)/1000.0\n",
    "    time.sleep(j) #waits for a random time so that the website don't consider you as a bot\n",
    "    if get_info:\n",
    "        soup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "        print('extracting %d job keywords...' % i)\n",
    "        soup_string = str(soup)\n",
    "        single_job = communication_skills_f(soup)\n",
    "        print(single_job,len(soup))\n",
    "        print(driver.current_url)\n",
    "        job_keywords.append([driver.current_url,single_job])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in range(1,num_pages + 1):\n",
    "# #this 5 pages reopen the browser is to prevent connection refused error.\n",
    "#     if k%5==0.0:\n",
    "#         driver.quit()\n",
    "#         driver=webdriver.Firefox(executable_path=r'C:\\Users\\emily\\Documents\\ML course\\geckodriver.exe')\n",
    "#         driver.set_page_load_timeout(15)\n",
    "\n",
    "#     current_url = start_url + \"&start=\" + str(k*10)\n",
    "#     print('extracting %d page of job searching results...' % k)\n",
    "#     resp = requests.get(current_url)\n",
    "#     current_soup = BeautifulSoup(resp.content, 'lxml')\n",
    "#     current_urls = current_soup.findAll('a',{'rel':'nofollow','target':'_blank'})\n",
    "#     current_urls = [link['href'] for link in current_urls]\n",
    "    \n",
    "#     for i in range(len(current_urls)):\n",
    "#         get_info = True\n",
    "#         try:\n",
    "#             driver.get(base_url + current_urls[i])\n",
    "#         except TimeoutException:\n",
    "#             get_info = False\n",
    "#             continue \n",
    "#         j = random.randint(1500,3200)/1000.0\n",
    "#         time.sleep(j) #waits for a random time\n",
    "#         if get_info:\n",
    "#             soup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "#             print('extracting %d job keywords...' % i)\n",
    "#             single_job = keywords_f(soup)\n",
    "#             print(single_job,len(soup))\n",
    "#             print(driver.current_url)\n",
    "#             job_keywords.append([driver.current_url,single_job])\n",
    "            \n",
    "# # use driver.quit() not driver.close() can get rid of the opening too many files error.\n",
    "# driver.quit()\n",
    "# skills_dict = [w[1] for w in job_keywords]\n",
    "# dict={}\n",
    "# for words in skills_dict:\n",
    "#     for word in words:\n",
    "#         if not word in dict:\n",
    "#             dict[word]=1\n",
    "#         else:\n",
    "#             dict[word]+=1\n",
    "# Result = pd.DataFrame()\n",
    "# Result['Skill'] = dict.keys()\n",
    "# Result['Count'] = dict.values()\n",
    "# Result['Ranking'] = Result['Count']/float(len(job_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Result.to_csv(\"skills_for_BIconsultant.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
